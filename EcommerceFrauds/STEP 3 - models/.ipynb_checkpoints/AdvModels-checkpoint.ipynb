{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba1f9b42-c61b-4e30-8443-1bd126d2351e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import json \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from catboost import Pool \n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from ColumnTransformers import *\n",
    "from sklearn import set_config\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eebb8187-2fee-4cc0-ad8a-5826ab8b2789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XgboostSearch(scale_pos_weight,df_X,y,metric=\"roc_auc\"): \n",
    "    param_grid_xgb = {\n",
    "        'n_estimators': [400,550,700],  \n",
    "        'learning_rate': [0.0015,0.01],  \n",
    "        'max_depth': [3,4,5,6], \n",
    "        'subsample': [1],  \n",
    "        'colsample_bytree': [1.0],  \n",
    "        'gamma': [0.2,0.4], \n",
    "        'reg_lambda': [1], \n",
    "        'scale_pos_weight':[scale_pos_weight,2*scale_pos_weight], \n",
    "        }  \n",
    "    Lr=PipeLineGradient() \n",
    "    Lr.fit(df_X,y) \n",
    "    X=Lr.transform(df_X) \n",
    "    model=XGBClassifier()\n",
    "    grid_search_xgb = GridSearchCV(model, param_grid_xgb, cv=5, scoring=metric, n_jobs=3)\n",
    "    grid_search_xgb.fit(X,y)\n",
    "    best_params=grid_search_xgb.best_params_\n",
    "    with open(\"savedModels/XGBClassifier.json\", \"w\") as f:\n",
    "        json.dump(best_params, f, indent=4)    \n",
    "    return  best_params\n",
    "\n",
    "def LightgbmSearch(df_X,y,metric=\"roc_auc\"):\n",
    "    param_grid_lgb = {\n",
    "        'n_estimators': [600,700,900],\n",
    "        'learning_rate': [0.01],\n",
    "        'max_depth': [2,3,4],\n",
    "        'subsample': [1],\n",
    "        'colsample_bytree': [0.8],\n",
    "        'min_split_gain': [0.1],  \n",
    "        'reg_lambda': [1,3], \n",
    "        'is_unbalance':[True] }\n",
    "\n",
    "    Lr=PipeLineGradient() \n",
    "    Lr.fit(df_X,y) \n",
    "    X=Lr.transform(df_X) \n",
    "    model=LGBMClassifier(verbosity=-1)\n",
    "    grid_search_lgb = GridSearchCV(model, param_grid_lgb, cv=5, scoring=metric, n_jobs=3)\n",
    "    grid_search_lgb.fit(X,y)\n",
    "    best_params=grid_search_lgb.best_params_\n",
    "    with open(\"savedModels/LGBMClassifier.json\", \"w\") as f:\n",
    "        json.dump(best_params, f, indent=4)\n",
    "    return  best_params\n",
    "\n",
    "def AdaSearch(df_X,y,metric='roc_auc') : \n",
    "    param_grid_ada = {\n",
    "    'n_estimators': [70,100,120], \n",
    "    'learning_rate': [0.1,0.3],  \n",
    "    'estimator': [DecisionTreeClassifier(max_depth=6), \n",
    "                          DecisionTreeClassifier(max_depth=7)\n",
    "                 ]\n",
    "    }\n",
    "    Lr=PipeLineGradient() \n",
    "    Lr.fit(df_X,y) \n",
    "    X=Lr.transform(df_X) \n",
    "    model= AdaBoostClassifier()\n",
    "    grid_search_ada = GridSearchCV(model, param_grid_ada, cv=5, scoring=metric, n_jobs=3)\n",
    "    grid_search_ada.fit(X,y)\n",
    "    best_params=grid_search_ada.best_params_\n",
    "    serializable_params = {k: str(v) for k, v in best_params.items()}\n",
    "    with open(\"savedModels/AdaBoost.json\", \"w\") as f:\n",
    "        json.dump(serializable_params, f, indent=4)\n",
    "    print(f\"Adaboost score: {123}\")\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb7414ee-cbd5-4174-8b99-7e4d521520fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadCreateModel(modelName): \n",
    "    if modelName!=\"CatBoostClassifier\":\n",
    "        with open(f\"savedModels/{modelName}.json\", \"r\") as f:\n",
    "            best_params=json.load(f) \n",
    "    if modelName == \"AdaBoost\":\n",
    "            if \"learning_rate\" in best_params:\n",
    "                best_params[\"learning_rate\"] = float(best_params[\"learning_rate\"])\n",
    "            if \"n_estimators\" in best_params:\n",
    "                best_params[\"n_estimators\"] = int(best_params[\"n_estimators\"])\n",
    "            if \"estimator\" in best_params and isinstance(best_params[\"estimator\"], str):\n",
    "                estimator_str = best_params.pop(\"estimator\")\n",
    "                if estimator_str.startswith(\"DecisionTreeClassifier\"):\n",
    "                    estimator = eval(estimator_str) \n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported estimator: {estimator_str}\")\n",
    "                createdModel = AdaBoostClassifier(estimator=estimator)\n",
    "            else:\n",
    "                createdModel = AdaBoostClassifier()\n",
    "    elif modelName==\"XGBClassifier\":\n",
    "        createdModel=XGBClassifier(random_state=42, verbosity=0, n_jobs=-1)\n",
    "    elif modelName==\"LGBMClassifier\":\n",
    "        createdModel=LGBMClassifier(random_state=42 ,n_jobs=-1,verbosity=-1)\n",
    "    else:   \n",
    "        with open(f\"savedModels/{modelName}.json\", \"r\") as f:\n",
    "            data=json.load(f) \n",
    "            best_params = data[\"params\"]\n",
    "        createdModel=CatBoostClassifier(random_state=42,verbose=0)\n",
    "    createdModel.set_params(**best_params) \n",
    "    return createdModel\n",
    "    \n",
    "def TestingModel(modelName,X_train,X_test,y_train,y_test,threshold ): \n",
    "    \n",
    "    Model=ReadCreateModel(modelName) \n",
    "    if modelName!=\"CatBoostClassifier\":\n",
    "        Model=PipelineModel(Model)\n",
    "        Model.fit(X_train,y_train) \n",
    "        y_scores=Model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        X_train_transformed,cat_train=MergeNumCat(X_train)\n",
    "        X_test_transformed,cat_test=MergeNumCat(X_test)\n",
    "        pool_train=Pool(data=X_train_transformed,label=y_train,cat_features=cat_train)\n",
    "        pool_test = Pool(data=X_test_transformed, cat_features=cat_test)\n",
    "        Model.fit(pool_train)\n",
    "        y_scores= Model.predict_proba(pool_test)[:,1]\n",
    "        \n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucPlot(fpr, tpr,roc_auc)\n",
    "\n",
    "    y_pred = (np.array(y_scores) > threshold).astype(int)\n",
    "    PredictionQualityInfo(y_pred,y_test)\n",
    "    print(f\"AUC: {roc_auc}, Recall: {recall_score(y_test,y_pred)}\")\n",
    "    return Model\n",
    "\n",
    "\n",
    "def aucPlot(fpr,tpr,roc_auc): \n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\") \n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28dd3937-a1d0-4795-987d-ac2d7f804713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MergeNumCat(X):\n",
    "    CatTransformer=CatBoostTransformer() \n",
    "    CatTransformer.fit(X) \n",
    "    X_transformed=CatTransformer.transform(X)\n",
    "    categorical=[col for col in range(X_transformed.shape[1]) if len(np.unique(X_transformed[:, col]))<6] \n",
    "    X_transformed[:, categorical]=X_transformed[:, categorical].astype(str)\n",
    "    return X_transformed,categorical\n",
    "    \n",
    "def GetCategorical(X):\n",
    "    categorical=[int(col) for col in range(X.shape[1]) if len(np.unique(X[:, col]))<6] \n",
    "    return categorical\n",
    "    \n",
    "def CatBoostSearch(X,y): \n",
    "    param_grid_cat = {\n",
    "    'iterations': [1400,1500,1600],           \n",
    "    'learning_rate': [0.01,0.015],    \n",
    "    'depth': [9,10,11],                    \n",
    "    'l2_leaf_reg': [2],                                        \n",
    "    'auto_class_weights':['Balanced'],         \n",
    "    'grow_policy': ['SymmetricTree'],  \n",
    "    'colsample_bylevel': [0.8],\n",
    "    'border_count': [128,256]\n",
    "    }\n",
    "    X_train,cat_features=MergeNumCat(X)\n",
    "    \n",
    "    train_pool = Pool(data=X_train, label=y, cat_features=cat_features)\n",
    "    \n",
    "    model = CatBoostClassifier(verbose=0)\n",
    "    \n",
    "    grid_result=model.grid_search(param_grid_cat, X=train_pool, y=None,cv=5,refit=True)\n",
    "    \n",
    "    with open(f\"savedModels/CatBoostClassifier.json\", \"w\") as f:\n",
    "        json.dump(grid_result, f, indent=4)\n",
    "    return  grid_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20ef693b-bc90-43c7-b46d-f1b028fe02f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GiantSearch(): \n",
    "    X,y=KCrossData()\n",
    "    CatBoostSearch(X,y)\n",
    "    XgboostSearch(GetScalePosWeight(y),X,y,metric='recall')\n",
    "    LightgbmSearch(X,y,metric='recall')\n",
    "    AdaSearch(X,y,metric='recall') \n",
    "    print(\"Well done\") \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ff15fb6-23b7-46d8-832d-a200eca70f51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well done\n"
     ]
    }
   ],
   "source": [
    "#GiantSearch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88389551-ad5d-40f9-bd15-2b5817a44f12",
   "metadata": {},
   "source": [
    "Modele na danych testowych po gridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887a2f04-b2c4-420b-857e-fbcf20f6d158",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train,y_train=getTestData()\n",
    "X_test,y_test=getTrainingData()\n",
    "TestingModel(\"XGBClassifier\",X_train,X_test,y_train,y_test,threshold=0.5)\n",
    "TestingModel(\"AdaBoost\",X_train,X_test,y_train,y_test,threshold=0.5)\n",
    "TestingModel(\"LGBMClassifier\",X_train,X_test,y_train,y_test,threshold=0.5)\n",
    "TestingModel(\"CatBoostClassifier\",X_train,X_test,y_train,y_test,threshold=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b7b5dc-26c9-40c8-a2ab-9f762545d068",
   "metadata": {},
   "source": [
    "Po wykonaniu 2-krotnego gridsearch na 4 powyzszych modelach, gdzie scoring byl ustawiony na auc, dostalismy nie zadowolajace efekty. Postanowilismy zmienic\n",
    "scoring na recall. Po zmianie i kolejnym grid searchu, modele LightBGM oraz CatBoost sprawdzaly sie na najlepiej, gdzie recall byl na poziomie 0.5-0.55. Ostatnie strojnie parametrow na dwoch najlepszych modelach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
